{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D2Arkxvv0qB",
        "outputId": "818f18f3-0612-4db0-fc26-4135f1fdcaa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/78.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m71.7/78.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.9/120.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.0/204.0 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.9/98.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU openimages torch_snippets urllib3\n",
        "!wget -O open_images_train_captions.jsonl -q https://storage.googleapis.com/localized-narratives/annotations/open_images_train_v6_captions.jsonl\n",
        "from torch_snippets import *\n",
        "import json\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZZUjkbIvp1E",
        "outputId": "8e56b1c7-03cf-4d67-dc38-63ddc091a52c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.4\n",
            "  Downloading torchtext-0.4.0-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.4) (4.66.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.4) (2.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.4) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.4) (1.23.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.4) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.4) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.4) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.4) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.4) (2023.11.17)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.4) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.4) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.4) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.4) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.4) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.4) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.4) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.4) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.4) (1.3.0)\n",
            "Installing collected packages: torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.16.0\n",
            "    Uninstalling torchtext-0.16.0:\n",
            "      Successfully uninstalled torchtext-0.16.0\n",
            "Successfully installed torchtext-0.4.0\n"
          ]
        }
      ],
      "source": [
        "#installing libraries\n",
        "%matplotlib inline\n",
        "!pip install torch>=1.3.1\n",
        "!pip install torchtext==0.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfdW99mOzo9l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWRqYOIo80k-"
      },
      "outputs": [],
      "source": [
        "#importing libraries\n",
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import AG_NEWS\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfuGLsmyppDE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "cdO_0l33mUH-",
        "outputId": "9b7b32bc-5b53-4e05-e9f1-212d2c6b9744"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running on device:  cuda\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Running on device:  cuda\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:03<00:00, 2515.35it/s]\n",
            "  0%|          | 0/9530 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './train-images/11775722863a7c7c.jpg'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-000f3ad683b4>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0msubset_imageIds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0m_download_images_by_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset_imageIds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./train-images/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0msubset_imageIds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openimages/download.py\u001b[0m in \u001b[0;36m_download_images_by_id\u001b[0;34m(image_ids, section, images_directory)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;31m# use the executor to map the download function to the iterable of arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         list(tqdm(executor.map(_download_single_image, download_args_list),\n\u001b[0m\u001b[1;32m    303\u001b[0m                   total=len(download_args_list)))\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    619\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0m_result_or_cancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0m_result_or_cancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openimages/download.py\u001b[0m in \u001b[0;36m_download_single_image\u001b[0;34m(arguments)\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dest_file_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdest_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m             arguments[\"s3_client\"].download_fileobj(\n\u001b[1;32m    762\u001b[0m                 \u001b[0;34m\"open-images-dataset\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './train-images/11775722863a7c7c.jpg'"
          ]
        }
      ],
      "source": [
        "# Import the relevant packages, define the device\n",
        "from torch_snippets import *\n",
        "import json\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(\"Running on device: \", device)\n",
        "\n",
        "# Loop through the content of the JSON file and fetch the information of the first 100,000 images\n",
        "with open('open_images_train_captions.jsonl', 'r') as json_file:\n",
        "    json_list = json_file.read().split('\\n')\n",
        "\n",
        "np.random.shuffle(json_list)\n",
        "data = []\n",
        "N = 10000 # Doing 10000 instead of 100000 because cuda runs out of memory\n",
        "\n",
        "for ix, json_str in Tqdm(enumerate(json_list), N):\n",
        "    if ix == N: break\n",
        "    try:\n",
        "        result = json.loads(json_str)\n",
        "        x = pd.DataFrame.from_dict(result, orient='index').T\n",
        "        data.append(x)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "# Split the dataframe (data) into training and validation datasets\n",
        "np.random.seed(10)\n",
        "\n",
        "data = pd.concat(data)\n",
        "data['train'] = np.random.choice([True,False], size=len(data),p=[0.95,0.05])\n",
        "data.to_csv('data.csv', index=False)\n",
        "\n",
        "\n",
        "# Download the images corresponding to the image IDs fetched from the JSON file:\n",
        "from openimages.download import _download_images_by_id\n",
        "\n",
        "subset_imageIds = data[data['train']].image_id.tolist()\n",
        "_download_images_by_id(subset_imageIds, 'train', './train-images/')\n",
        "\n",
        "subset_imageIds = data[~data['train']].image_id.tolist()\n",
        "_download_images_by_id(subset_imageIds, 'train', './val-images/')\n",
        "\n",
        "\n",
        "# A vocabulary object is something that can map every word in all the captions to a unique integer and vice versa\n",
        "from torchtext.legacy.data import Field\n",
        "from pycocotools.coco import COCO\n",
        "from collections import defaultdict\n",
        "\n",
        "captions = Field(sequential=False, init_token='', eos_token='')\n",
        "\n",
        "all_captions = data[data['train']]['caption'].tolist()\n",
        "all_tokens = [[w.lower() for w in c.split()] for c in all_captions]\n",
        "all_tokens = [w for sublist in all_tokens for w in sublist]\n",
        "\n",
        "captions.build_vocab(all_tokens)\n",
        "\n",
        "\n",
        "# Captions vocabulary components\n",
        "class Vocab: pass\n",
        "\n",
        "vocab = Vocab()\n",
        "captions.vocab.itos.insert(0, '')\n",
        "vocab.itos = captions.vocab.itos\n",
        "\n",
        "vocab.stoi = defaultdict(lambda:captions.vocab.itos.index(''))\n",
        "vocab.stoi[''] = 0\n",
        "\n",
        "for s,i in captions.vocab.stoi.items():\n",
        "    vocab.stoi[s] = i+1\n",
        "\n",
        "\n",
        "# Dataset class\n",
        "from torchvision import transforms\n",
        "\n",
        "class CaptioningData(Dataset):\n",
        "    def __init__(self, root, df, vocab):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.root = root\n",
        "        self.vocab = vocab\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.RandomCrop(224),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))]\n",
        "        )\n",
        "\n",
        "    # Returns one data pair (image and caption)\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index].squeeze()\n",
        "        id = row.image_id\n",
        "\n",
        "        image_path = f'{self.root}/{id}.jpg'\n",
        "        image = Image.open(os.path.join(image_path)).convert('RGB')\n",
        "\n",
        "        caption = row.caption\n",
        "\n",
        "        tokens = str(caption).lower().split()\n",
        "        target = []\n",
        "        target.append(vocab.stoi[''])\n",
        "        target.extend([vocab.stoi[token] for token in tokens])\n",
        "        target.append(vocab.stoi[''])\n",
        "        target = torch.Tensor(target).long()\n",
        "\n",
        "        return image, target, caption\n",
        "\n",
        "    def choose(self):\n",
        "        return self[np.random.randint(len(self))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    # Creates batch of captions and padds captions to be equal length\n",
        "    def collate_fn(self, data):\n",
        "        data.sort(key=lambda x: len(x[1]), reverse=True)\n",
        "        images, targets, captions = zip(*data)\n",
        "        images = torch.stack([self.transform(image) for image in images], 0)\n",
        "\n",
        "        lengths = [len(tar) for tar in targets]\n",
        "        _targets = torch.zeros(len(captions), max(lengths)).long()\n",
        "\n",
        "        for i, tar in enumerate(targets):\n",
        "            end = lengths[i]\n",
        "            _targets[i, :end] = tar[:end]\n",
        "\n",
        "        return images.to(device), _targets.to(device), torch.tensor(lengths).long().to(device)\n",
        "\n",
        "\n",
        "#Define the training and validation dataset and data loaders\n",
        "trn_ds = CaptioningData('train-images', data[data['train']], vocab)\n",
        "\n",
        "val_ds = CaptioningData('val-images', data[~data['train']], vocab)\n",
        "\n",
        "# Show sample image and caption\n",
        "image, target, caption = trn_ds.choose()\n",
        "show(image, title=caption, sz=5); print(target)\n",
        "\n",
        "\n",
        "# Create the dataloaders for the datasets\n",
        "trn_dl = DataLoader(trn_ds, 32, collate_fn=trn_ds.collate_fn)\n",
        "val_dl = DataLoader(val_ds, 32, collate_fn=val_ds.collate_fn)\n",
        "\n",
        "# A sample batch\n",
        "inspect(*next(iter(trn_dl)), names='images,targets,lengths')\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from torchvision import models\n",
        "\n",
        "# The network class - EncoderCNN\n",
        "class EncoderCNN(nn.Module):\n",
        "\n",
        "    # Load the pretrained ResNet-152 and replace top fc layer\n",
        "    def __init__(self, embed_size):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet152(pretrained=True)\n",
        "\n",
        "        # Felete the last fc layer.\n",
        "        modules = list(resnet.children())[:-1]\n",
        "\n",
        "        # Connect it to a linear layer\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "\n",
        "        # Pass it through batch normalization\n",
        "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
        "\n",
        "    # Extract feature vectors from input images\n",
        "    def forward(self, images):\n",
        "        with torch.no_grad():\n",
        "            features = self.resnet(images)\n",
        "\n",
        "        features = features.reshape(features.size(0), -1)\n",
        "        features = self.bn(self.linear(features))\n",
        "\n",
        "        return features\n",
        "\n",
        "# Creating encoder instance\n",
        "encoder = EncoderCNN(256).to(device)\n",
        "\n",
        "\n",
        "# Define the decoder architecture – DecoderRNN\n",
        "class DecoderRNN(nn.Module):\n",
        "\n",
        "    # Set the hyper-parameters and build the layers\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=80):\n",
        "\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    # Decode image feature vectors and generates captions\n",
        "    def forward(self, features, captions, lengths):\n",
        "        embeddings = self.embed(captions)\n",
        "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
        "\n",
        "        packed = pack_padded_sequence(embeddings, lengths.cpu(), batch_first=True)\n",
        "\n",
        "        outputs, _ = self.lstm(packed)\n",
        "        outputs = self.linear(outputs[0])\n",
        "        return outputs\n",
        "\n",
        "    #Generate captions for given image features using greedy search\n",
        "    def predict(self, features, states=None):\n",
        "        sampled_ids = []\n",
        "        inputs = features.unsqueeze(1)\n",
        "\n",
        "        for i in range(self.max_seq_length):\n",
        "            hiddens, states = self.lstm(inputs, states) # hiddens: (batch_size, 1, hidden_size)\n",
        "\n",
        "            outputs = self.linear(hiddens.squeeze(1)) # outputs: (batch_size, vocab_size)\n",
        "            _, predicted = outputs.max(1) # predicted: (batch_size)\n",
        "\n",
        "            sampled_ids.append(predicted)\n",
        "\n",
        "            inputs = self.embed(predicted) # inputs: (batch_size, embed_size)\n",
        "            inputs = inputs.unsqueeze(1) # inputs: (batch_size, 1, embed_size)\n",
        "\n",
        "\n",
        "        sampled_ids = torch.stack(sampled_ids, 1) # sampled_ids: (batch_size, max_seq_length)\n",
        "\n",
        "        # convert predicted tokens to strings\n",
        "        sentences = []\n",
        "        for sampled_id in sampled_ids:\n",
        "            sampled_id = sampled_id.cpu().numpy()\n",
        "            sampled_caption = []\n",
        "\n",
        "            for word_id in sampled_id:\n",
        "                word = vocab.itos[word_id]\n",
        "                sampled_caption.append(word)\n",
        "\n",
        "                if word == '':\n",
        "                    break\n",
        "\n",
        "            sentence = ' '.join(sampled_caption)\n",
        "            sentences.append(sentence)\n",
        "\n",
        "        return sentences\n",
        "\n",
        "\n",
        "\n",
        "# Trains on a single batch of data\n",
        "def train_batch(data, encoder, decoder, optimizer, criterion):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    images, captions, lengths = data\n",
        "    images = images.to(device)\n",
        "\n",
        "    captions = captions.to(device)\n",
        "    targets = pack_padded_sequence(captions, lengths.cpu(), batch_first=True)[0]\n",
        "\n",
        "    features = encoder(images)\n",
        "    outputs = decoder(features, captions, lengths)\n",
        "    loss = criterion(outputs, targets)\n",
        "\n",
        "    decoder.zero_grad()\n",
        "    encoder.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "# Validate on a batch of data\n",
        "@torch.no_grad()\n",
        "def validate_batch(data, encoder, decoder, criterion):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    images, captions, lengths = data\n",
        "    images = images.to(device)\n",
        "\n",
        "    captions = captions.to(device)\n",
        "    targets = pack_padded_sequence(captions, lengths.cpu(), batch_first=True)[0]\n",
        "\n",
        "    features = encoder(images)\n",
        "    outputs = decoder(features, captions, lengths)\n",
        "\n",
        "    loss = criterion(outputs, targets)\n",
        "    return loss\n",
        "\n",
        "\n",
        "# Define encoder, decoder, loss function, and optimizer\n",
        "encoder = EncoderCNN(256).to(device)\n",
        "decoder = DecoderRNN(256, 512, len(vocab.itos), 1).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "params = list(decoder.parameters()) + \\\n",
        "         list(encoder.linear.parameters()) + \\\n",
        "         list(encoder.bn.parameters())\n",
        "\n",
        "optimizer = torch.optim.AdamW(params, lr=1e-3)\n",
        "\n",
        "n_epochs = 5 # Doing 5 instead of 10 because training on CPU\n",
        "log = Report(n_epochs)\n",
        "\n",
        "\n",
        "# Train the model over increasing epochs\n",
        "for epoch in range(n_epochs):\n",
        "    if epoch == 3: optimizer = torch.optim.AdamW(params, lr=1e-4)\n",
        "\n",
        "    N = len(trn_dl)\n",
        "    for i, data in enumerate(trn_dl):\n",
        "        trn_loss = train_batch(data, encoder, decoder, optimizer, criterion)\n",
        "        pos = epoch + (1+i)/N\n",
        "        log.record(pos=pos, trn_loss=trn_loss, end='\\r')\n",
        "\n",
        "    N = len(val_dl)\n",
        "\n",
        "    for i, data in enumerate(val_dl):\n",
        "        val_loss = validate_batch(data, encoder, decoder, criterion)\n",
        "        pos = epoch + (1+i)/N\n",
        "        log.record(pos=pos, val_loss=val_loss, end='\\r')\n",
        "\n",
        "    log.report_avgs(epoch+1)\n",
        "\n",
        "log.plot_epochs(log=True)\n",
        "\n",
        "\n",
        "\n",
        "# Generates predictions given an image\n",
        "def load_image(image_path, transform=None):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = image.resize([224, 224], Image.LANCZOS)\n",
        "\n",
        "    if transform is not None:\n",
        "        tfm_image = transform(image)[None]\n",
        "\n",
        "    return image, tfm_image\n",
        "\n",
        "def load_image_and_predict(image_path):\n",
        "    transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n",
        "                                    ])\n",
        "\n",
        "    org_image, tfm_image = load_image(image_path, transform)\n",
        "    image_tensor = tfm_image.to(device)\n",
        "\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    feature = encoder(image_tensor)\n",
        "    sentence = decoder.predict(feature)[0]\n",
        "\n",
        "    show(org_image, title=sentence)\n",
        "    return sentence\n",
        "\n",
        "# Predict an image\n",
        "files = Glob('val-images')\n",
        "load_image_and_predict(choose(files))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NVavrYm10Jm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDhbgXn60Mv6"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxDHNls90NxS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9abtsg7q0Aig"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_IL7W2pfnC6"
      },
      "outputs": [],
      "source": [
        "print(torchtext.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "g2fID7GX7lU_",
        "outputId": "c1d1da8e-e760-49c6-e4d1-54c1ea34eb1a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e9792d4aa3b3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mNGRAMS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./.data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./.data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n\u001b[1;32m      5\u001b[0m     root='./.data', ngrams=NGRAMS, vocab=None)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ],
      "source": [
        "NGRAMS = 1\n",
        "if not os.path.isdir('./.data'):\n",
        "\tos.mkdir('./.data')\n",
        "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
        "    root='./.data', ngrams=NGRAMS, vocab=None)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSuAX5YH80lC"
      },
      "outputs": [],
      "source": [
        "C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmosUHpG80lG",
        "outputId": "a60ba24c-3f17-4a9a-e4c7-5aeee73a9777",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8386a814eede>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTextSentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbeddingBag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_class\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Use linear layer here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "class TextSentiment(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Transformer(embed_dim, num_class) # Use linear layer here.\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):  # Randomly initilaize parameters\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange) # Uniform distribution\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()  # Make bias zeros\n",
        "\n",
        "    def forward(self, text, offsets): # Foward pass\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        # input (N,)  it will be treated as a concatenation of multiple bags (sequences).\n",
        "        # offsets is required to be a 1D tensor containing the starting index positions of each bag in input.\n",
        "        # Therefore, for offsets of shape (B), input will be viewed as having B bags.\n",
        "        # ouput (B, embed_dim)\n",
        "        #print(\"embed\", embedded.shape)\n",
        "        a =  self.fc(embedded)\n",
        "        #print('linear', a.shape)\n",
        "        return a # ouput (B, num_class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuQMytxO80lK"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
        "EMBED_DIM = 32\n",
        "NUM_CLASS = len(train_dataset.get_labels())\n",
        "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUM_CLASS).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POrdK4KQ80lN"
      },
      "source": [
        "Functions used to generate batch\n",
        "--------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xz5A--Hq80lO"
      },
      "outputs": [],
      "source": [
        "def generate_batch(batch):\n",
        "    # Input: a iterator of items with length of batch_size. For example:[(1,(tensor([2,4,3])),(0,tensor([6,5]))]\n",
        "    # Generate a batch used in SGD\n",
        "    label = torch.tensor([entry[0] for entry in batch]) #tensor of shape (batch_size,)\n",
        "    text = [entry[1] for entry in batch]\n",
        "    offsets = [0] + [len(entry) for entry in text]\n",
        "    # torch.Tensor.cumsum returns the cumulative sum\n",
        "    # of elements in the dimension dim.\n",
        "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
        "    #print(\"!!!!\")\n",
        "    #print(offsets)\n",
        "\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0) # For example tensor([0,3])\n",
        "    #print(offsets)\n",
        "    text = torch.cat(text) # a list of tensor -> tensor of shape (sum([len(i) for i in text]),) For example, tensor([2,4,3,6,5])\n",
        "    return text, offsets, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "913drrJO80lR"
      },
      "source": [
        "Define functions to train the model and evaluate results.\n",
        "---------------------------------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPjVXDQj80lT"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "def train_func(sub_train_):\n",
        "\n",
        "    # Train the model\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                      collate_fn=generate_batch)  # Iterable batches\n",
        "    for i, (text, offsets, cls) in enumerate(data):\n",
        "        optimizer.zero_grad() # Before each optimization, make previous gradients zeros\n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "        output = model(text, offsets)\n",
        "        loss = criterion(output, cls) # Forward pass to compute loss\n",
        "        train_loss += loss.item()\n",
        "        # Extract the number from a tensor containing only one item, this number will be used in later printing\n",
        "        loss.backward() # Backforward propagation to compute gradients of each variable node\n",
        "        optimizer.step() # Update parameters according to gradients\n",
        "        #choose the class with the highest score as current prediction and compare with gold label (cls )\n",
        "        train_acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "    # Adjust the learning rate. After each epoch, do learning rate decay ( optional )\n",
        "    scheduler.step()\n",
        "\n",
        "    return train_loss / len(sub_train_), train_acc / len(sub_train_) #return average loss and acc to print\n",
        "\n",
        "def test(data_):\n",
        "    #Similar to train_func but do not need back propagation or parameter update !\n",
        "    loss = 0\n",
        "    acc = 0\n",
        "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
        "    for text, offsets, cls in data:\n",
        "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
        "        with torch.no_grad(): # prevent computing gradient, could not use backward()\n",
        "            output = model(text, offsets)\n",
        "            loss = criterion(output, cls)\n",
        "            loss += loss.item()\n",
        "            acc += (output.argmax(1) == cls).sum().item()\n",
        "\n",
        "    return loss / len(data_), acc / len(data_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KN0qicW980lW"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from torch.utils.data.dataset import random_split\n",
        "N_EPOCHS = 30\n",
        "min_valid_loss = float('inf')\n",
        "\n",
        "#Use CrossEntropyLoss() as the criterion.\n",
        "#The input is the output of the model. First do logsoftmax, then compute cross-entropy loss.\n",
        "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
        "#Use SGD as optimizer.\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=4)\n",
        "#Use exponential decay to decrease learning rate\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 0.1, gamma=0.9)\n",
        "\n",
        "train_len = int(len(train_dataset) * 0.95)\n",
        "#Split whole training dataset to create validation (hold-out datset)\n",
        "sub_train_, sub_valid_ = \\\n",
        "    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_loss, train_acc = train_func(sub_train_)\n",
        "    valid_loss, valid_acc = test(sub_valid_)\n",
        "\n",
        "    secs = int(time.time() - start_time)\n",
        "    mins = secs / 60\n",
        "    secs = secs % 60\n",
        "\n",
        "    #Print information to monitor the training process\n",
        "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
        "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0AY2HBG80la"
      },
      "source": [
        "Evaluate the model with test dataset\n",
        "------------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpKB9dK_80lb"
      },
      "outputs": [],
      "source": [
        "print('Checking the results of test dataset...')\n",
        "test_loss, test_acc = test(test_dataset)\n",
        "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iEShjoB80le"
      },
      "source": [
        "Checking the results of test dataset…\n",
        "\n",
        "::\n",
        "\n",
        "       Loss: 0.0237(test)      |       Acc: 90.5%(test)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kCdwqnH80lf"
      },
      "source": [
        "Test on a random news\n",
        "---------------------\n",
        "\n",
        "Use the best model so far and test a golf news. The label information is\n",
        "available\n",
        "`here <https://pytorch.org/text/datasets.html?highlight=ag_news#torchtext.datasets.AG_NEWS>`__.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_BKUall80lg"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from torchtext.data.utils import ngrams_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "ag_news_label = {1 : \"World\",\n",
        "                 2 : \"Sports\",\n",
        "                 3 : \"Business\",\n",
        "                 4 : \"Sci/Tec\"}\n",
        "\n",
        "def predict(text, model, vocab, ngrams):\n",
        "    tokenizer = get_tokenizer(\"basic_english\")\n",
        "    with torch.no_grad():\n",
        "        #tokenize, generate ngram list and use vocabulary to numericalize\n",
        "        text = torch.tensor([vocab[token]\n",
        "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
        "        print(text)\n",
        "        output = model(text, torch.tensor([0]))\n",
        "        # choose the class with the highest score as prediction\n",
        "        return output.argmax(1)..item() + 1\n",
        "\n",
        "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
        "    enduring the season’s worst weather conditions on Sunday at The \\\n",
        "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
        "    consid ering the wind and the rain was a respectable showing. \\\n",
        "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
        "    was another story. With temperatures in the mid-80s and hardly any \\\n",
        "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
        "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
        "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
        "    was even more impressive considering he’d never played the \\\n",
        "    front nine at TPC Southwind.\"\n",
        "\n",
        "vocab = train_dataset.get_vocab()\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, model, vocab, 1)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r6gVqaX80lk"
      },
      "source": [
        "This is a Sports news\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}